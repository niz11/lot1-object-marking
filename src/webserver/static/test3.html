<!doctype html>
<html>

<head>
    <meta charset="UTF-8">
    <meta name="viewport"
          content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <title>Hello WebXR!</title>

    <!-- three.js -->
    <script src="new/three.js"></script>
    <script src="new/GLTFLoader.js"></script>

    <script src="new/EventDispatcher.js"></script>

    <script src="jsartoolkit5/artoolkit.min.js"></script>
    <script src="jsartoolkit5/artoolkit.api.js"></script>
    <!-- include threex.artoolkit -->
    <script src="threex/threex-artoolkitsource.js"></script>
    <script src="threex/threex-artoolkitcontext.js"></script>
    <script src="threex/threex-arbasecontrols.js"></script>
    <script src="threex/threex-armarkercontrols.js"></script>
</head>

<body>

<!-- Starting an immersive WebXR session requires user interaction.
We start this one with a simple button. -->
<button onclick="activateXR()">Start Hello WebXR</button>
<!-- <script type="module">
    import {WebXRButton} from './webxr-button.js';


</script> -->
<script>

    let offCanvas = new OffscreenCanvas(window.innerWidth, window.innerHeight);
    let worker = new Worker('offscreenworker.js');


    // Add a canvas element and initialize a WebGL context that is compatible with WebXR.
    const canvas = document.createElement("canvas");
    document.body.appendChild(canvas);
    const gl = canvas.getContext("webgl", { xrCompatible: true });

    const scene = new THREE.Scene();

    const directionalLight = new THREE.DirectionalLight(0xffffff, 0.3);
    directionalLight.position.set(10, 15, 10);
    scene.add(directionalLight);

    // Set up the WebGLRenderer, which handles rendering to the session's base layer.
    const renderer = new THREE.WebGLRenderer({
        alpha: true,
        preserveDrawingBuffer: true,
        canvas: canvas,
        context: gl
    });
    renderer.autoClear = false;


    async function activateXR() {
        console.log("Hi there");

        const camera = new THREE.PerspectiveCamera();
        camera.matrixAutoUpdate = false;

        const session = await navigator.xr.requestSession("immersive-ar", { requiredFeatures: ['hit-test', 'camera-access'] });
        session.updateRenderState({
            baseLayer: new XRWebGLLayer(session, gl)
        });

        const referenceSpace = await session.requestReferenceSpace('local');
        const viewerSpace = await session.requestReferenceSpace('viewer');
        const hitTestSource = await session.requestHitTestSource({ space: viewerSpace });
        const loader = new THREE.GLTFLoader();
        let reticle;
        loader.load("https://immersive-web.github.io/webxr-samples/media/gltf/reticle/reticle.gltf", function (gltf) {
            reticle = gltf.scene;
            reticle.visible = false;
            scene.add(reticle);
        });

        let flower;
        loader.load("https://immersive-web.github.io/webxr-samples/media/gltf/sunflower/sunflower.gltf", function (gltf) {
            flower = gltf.scene;
        });

        session.addEventListener("select", (event) => {
            if (flower) {
                flower.position.copy(reticle.position);
                scene.add(flower);
            }
        });

        // Create a render loop that allows us to draw on the AR view.
        const onXRFrame = (time, frame) => {
            // Queue up the next draw request.
            session.requestAnimationFrame(onXRFrame);

            // Bind the graphics framebuffer to the baseLayer's framebuffer
            gl.bindFramebuffer(gl.FRAMEBUFFER, session.renderState.baseLayer.framebuffer)

            // Retrieve the pose of the device.
            // XRFrame.getViewerPose can return null while the session attempts to establish tracking.
            const pose = frame.getViewerPose(referenceSpace);
            if (pose) {
                // In mobile AR, we only have one view.
                const view = pose.views[0];

                const viewport = session.renderState.baseLayer.getViewport(view);
                renderer.setSize(viewport.width, viewport.height);

                // Use the view's transform matrix and projection matrix to configure the THREE.camera.
                camera.matrix.fromArray(view.transform.matrix);
                camera.projectionMatrix.fromArray(view.projectionMatrix);
                camera.updateMatrixWorld(true);


                let tex = glBinding.getCameraImage(frame, view);
                worker.postMessage({canvas: offCanvas, artContext: arToolkitContext, texture: tex}, [offCanvas, arToolkitContext, tex]);

                //If marker was deected do hit test here
                const hitTestResults = frame.getHitTestResults(hitTestSource);
                if (hitTestResults.length > 0 && reticle) {
                    const hitPose = hitTestResults[0].getPose(referenceSpace);
                    reticle.visible = true;
                    reticle.position.set(hitPose.transform.position.x, hitPose.transform.position.y, hitPose.transform.position.z);
                    reticle.updateMatrixWorld(true);
                }

                // Render the scene with THREE.WebGLRenderer.
                renderer.render(scene, camera)
            }
        };
        session.requestAnimationFrame(onXRFrame);
        // initialize();
        // animate();
    }

    /** ARJS Initialization **/
    var arJSScene, arJSScamera, arJSScamera2, arJSSmesh, arJSSmarkerControls;
    var arToolkitSource, arToolkitContext;

    var firstUpdate = false;
    function initialize() {
        arJSScene = new THREE.Scene();

        let ambientLight = new THREE.AmbientLight(0xcccccc, 0.5);
        arJSScene.add(ambientLight);

        arJSScamera = new THREE.OrthographicCamera(window.innerWidth / -2, window.innerWidth / 2, window.innerHeight / -2, window.innerHeight / 2, -100, 100);
        arJSScene.add(arJSScamera);

        arJSSmesh = new THREE.Mesh(
            new THREE.BoxGeometry(5, 10, 10),
            new THREE.MeshBasicMaterial({color: 0xff0000})
        );
        arJSScene.add(arJSSmesh);
        arJSScamera.position.z = 5;

        arToolkitSource = new THREEx.ArToolkitSource({
            sourceType: 'webcam',
            showVideo: true
        });

        arToolkitSource.init(function onReady() {
            onResize()
        });

        // handle resize
        window.addEventListener('resize', function () {
            onResize()
        });

        window.addEventListener('arjs-video-loaded', function () {
            onResize()
        });

        function onResize() {
            arToolkitSource.onResizeElement();
            arToolkitSource.copyElementSizeTo(renderer.domElement)
            if (arToolkitContext.arController !== null) {
                arToolkitSource.copyElementSizeTo(arToolkitContext.arController.canvas)
            }
        }


        arToolkitContext = new THREEx.ArToolkitContext({
            cameraParametersUrl: 'data/camera_para.dat',
            detectionMode: 'mono',
            canvasWidth: 80 * 3,
            canvasHeight: 60 * 3,
            maxDetectionRate: 30
        });

        arJSScamera2 = new THREE.Camera();
        arToolkitContext.init(function onCompleted() {
            arJSScamera2.projectionMatrix.copy(arToolkitContext.getProjectionMatrix());
        });

        let patternArray = ["letterA", "letterB", "letterC", "letterD", "letterF", "kanji", "hiro"];
        for (let i = 6; i < 7; i++) {
            let markerRoot = new THREE.Group();
            arJSSmarkerControls = new THREEx.ArMarkerControls(arToolkitContext, markerRoot, {
                type: 'pattern', patternUrl: "data/" + patternArray[i] + ".patt",
            });
        }
    }



    window.addEventListener("markerFound", function (e) {
        console.log("Found")
        console.log(e.detail);
    });
    window.addEventListener("markerLost", function (e) {
        console.log("Lost")
        console.log(e.detail);
    });


    function update() {
        // console.log('upd');
        if (arToolkitSource.ready !== false) {
            arToolkitContext.update(arToolkitSource.domElement);
            if (!firstUpdate) {
                window.dispatchEvent(new CustomEvent('resize'));
                firstUpdate = true;
            }
        }

        if (arJSSmarkerControls.object3d.visible) {
            let pos = arJSSmarkerControls.object3d.position;
            let cpos = new THREE.Vector3(pos.x, pos.y, pos.z).project(arJSScamera2);
            // document.getElementById('mcx').innerText = 'x: ' + cpos.x;
            // document.getElementById('mcy').innerText = 'y: ' + cpos.y;
            // document.getElementById('mcz').innerText = 'z: ' + cpos.z;

            arJSSmesh.position.x = cpos.x * window.innerWidth / 2;
            arJSSmesh.position.y = cpos.y * window.innerHeight / -2;
        }

        // document.getElementById('mcw').innerText = 'visible: ' + markerControls.object3d.visible;
    }

    function animate() {
        requestAnimationFrame(animate);
        update();
        // renderer.render(arJSScene, arJSScamera);
    }
    // initialize();
    // animate();
</script>
</body>

</html>